{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIJw1Sco9Dlv",
        "outputId": "f65340f7-6ac9-451b-c4c5-ddfd6673173e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STJ03tGGpq-0"
      },
      "outputs": [],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "26ay_pYIqE2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.10.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: torchtext>=0.10.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.14.1)\n",
            "Requirement already satisfied: torchvision>=0.11.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: tokenizers>=0.10.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: Pillow>=2.1.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (9.5.0)\n",
            "Requirement already satisfied: transformers in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (4.30.2)\n",
            "Requirement already satisfied: Flask in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (2.2.5)\n",
            "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in .\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: tqdm in .\\.venv\\lib\\site-packages (from torchtext>=0.10.0->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in .\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in .\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in .\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (2023.10.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in .\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in .\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (6.7.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in .\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 8)) (2.2.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in .\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in .\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 8)) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in .\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 8)) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests->-r requirements.txt (line 9)) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests->-r requirements.txt (line 9)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests->-r requirements.txt (line 9)) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests->-r requirements.txt (line 9)) (2023.7.22)\n",
            "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from click>=8.0->Flask->-r requirements.txt (line 8)) (0.4.6)\n",
            "Requirement already satisfied: fsspec in .\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 7)) (2023.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in .\\.venv\\lib\\site-packages (from importlib-metadata->transformers->-r requirements.txt (line 7)) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\lib\\site-packages (from Jinja2>=3.0->Flask->-r requirements.txt (line 8)) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dVroT8STqSck"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\work\\image2dsl\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30512\\2707079159.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mtrain_data_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad_collate_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[0mval_data_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad_collate_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\work\\image2dsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# map-style\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\work\\image2dsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[1;32m--> 108\u001b[1;33m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from model import GPT2DecoderWithImageFeatures\n",
        "from torchvision import transforms\n",
        "\n",
        "vocabulary = ', { } small-title text quadruple row btn-inactive btn-orange btn-green btn-red double <START> header btn-active <END> single <UNK> <PAD>'.split()\n",
        "special_tokens_dict = {'additional_special_tokens': vocabulary}\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load and preprocess data\n",
        "class Pix2CodeDataset(Dataset):\n",
        "    def __init__(self, data_path, img_transform, dsl_transform, mode=\"train\", split_ratio=0.8):\n",
        "        self.data_path = data_path\n",
        "        self.img_transform = img_transform\n",
        "        self.dsl_transform = dsl_transform\n",
        "        self.mode = mode\n",
        "        self.split_ratio = split_ratio\n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        data = []\n",
        "        for root, _, files in os.walk(self.data_path):\n",
        "            for file in files:\n",
        "                if file.endswith(\".png\"):\n",
        "                    img_path = os.path.join(root, file)\n",
        "                    dsl_path = os.path.splitext(img_path)[0] + \".gui\"\n",
        "                    data.append((img_path, dsl_path))\n",
        "\n",
        "        split_index = int(len(data) * self.split_ratio)\n",
        "        if self.mode == \"train\":\n",
        "            return data[:split_index]\n",
        "        elif self.mode == \"val\":\n",
        "            return data[split_index:]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode. Use 'train' or 'val'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, dsl_path = self.data[idx]\n",
        "        img_rgb = Image.open(img_path)\n",
        "        img_grey = img_rgb.convert(\"L\")\n",
        "        img_adapted = img_grey.point(lambda x: 255 if x > 128 else 0)\n",
        "        img_stacked = np.stack((img_adapted, img_adapted, img_adapted), axis=-1)\n",
        "        img_stacked_pil = Image.fromarray(np.uint8(img_stacked), mode='RGB')\n",
        "\n",
        "        with open(dsl_path, \"r\") as f:\n",
        "            dsl_code = f.read()\n",
        "\n",
        "        img_tensor = self.img_transform(img_stacked_pil)\n",
        "        dsl_tokens = self.dsl_transform('<START>\\n' + dsl_code + '\\n<END>')\n",
        "        dsl_tensor = torch.LongTensor(dsl_tokens)\n",
        "\n",
        "        return img_tensor, dsl_tensor\n",
        "\n",
        "# Initialize ViT model, dataset, and data loader\n",
        "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').base_model\n",
        "\n",
        "# Replace the dsl_transform with the tokenizer.encode method\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "dsl_transform = tokenizer.encode\n",
        "\n",
        "# Create train and validation data loaders\n",
        "data_path = \"data/all_data/data\"\n",
        "\n",
        "train_dataset = Pix2CodeDataset(data_path, img_transform, dsl_transform, mode=\"train\")\n",
        "val_dataset = Pix2CodeDataset(data_path, img_transform, dsl_transform, mode=\"val\")\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    imgs, dsls = zip(*batch)\n",
        "\n",
        "    # Pad DSL sequences\n",
        "    max_len = max([len(dsl) for dsl in dsls])\n",
        "    end_token = tokenizer.encode('<PAD>')[0]\n",
        "    padded_dsls = []\n",
        "    for dsl in dsls:\n",
        "        padded_dsls.append(torch.cat([dsl, torch.full((max_len - len(dsl),), end_token, dtype=torch.long        )]))\n",
        "\n",
        "    # Stack padded DSL sequences and images\n",
        "    img_tensor = torch.stack(imgs)\n",
        "    dsl_tensor = torch.stack(padded_dsls)\n",
        "\n",
        "    return img_tensor, dsl_tensor\n",
        "\n",
        "batch_size = 32\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn)\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 768\n",
        "num_layers = 6\n",
        "epochs = 1000\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Initialize the decoder, loss function, and optimizer\n",
        "decoder = GPT2DecoderWithImageFeatures(input_size)\n",
        "decoder.gpt.resize_token_embeddings(len(tokenizer))  # Update the GPT2 model with the new tokenizer\n",
        "decoder.load_state_dict(torch.load(\"best_decoder.pth\"))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "# Initialize the learning rate scheduler with warm-up\n",
        "num_warmup_steps = 500\n",
        "num_training_steps = epochs * len(train_data_loader)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "# Initialize variables to track the best validation loss and epoch\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = 0\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "vit_model.eval()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (img_tensor, dsl_tensor) in enumerate(train_data_loader):\n",
        "        loss = 0\n",
        "        decoder.train()\n",
        "        img_tensor = img_tensor.to(device)\n",
        "        dsl_tensor = dsl_tensor.to(device)\n",
        "\n",
        "        # Extract image features using ViT model\n",
        "        with torch.no_grad():\n",
        "            image_features = vit_model(img_tensor).last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Training loop for the GPT2DecoderWithImageFeatures\n",
        "        input_tokens = dsl_tensor[:, :-1]\n",
        "        target_tokens = dsl_tensor[:, 1:]\n",
        "       \n",
        "        output = decoder(input_tokens, image_features)\n",
        "\n",
        "        # print(output.shape)\n",
        "        # print(target_tokens.shape)\n",
        "\n",
        "        loss = criterion(output.permute(0, 2, 1), target_tokens)\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Step {i}/{len(train_data_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (img_tensor, dsl_tensor) in enumerate(val_data_loader):\n",
        "            img_tensor = img_tensor.to(device)\n",
        "            dsl_tensor = dsl_tensor.to(device)\n",
        "\n",
        "            image_features = vit_model(img_tensor).last_hidden_state[:, 0, :]\n",
        "\n",
        "            input_tokens = dsl_tensor[:, :-1]\n",
        "            target_tokens = dsl_tensor[:, 1:]     \n",
        "\n",
        "            output = decoder(input_tokens, image_features)\n",
        "\n",
        "            val_loss += criterion(output.permute(0, 2, 1), target_tokens).item()\n",
        "\n",
        "    val_loss /= len(val_data_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss}\")\n",
        "    # Save the best model weights\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save(decoder.state_dict(), \"best_decoder.pth\")\n",
        "\n",
        "print(f\"Best model weights saved from epoch {best_epoch+1} with validation loss {best_val_loss}\")\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gte0ZiUuhT3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "from PIL import Image\n",
        "from model import GPT2DecoderWithImageFeatures\n",
        "from transformers import ViTModel\n",
        "\n",
        "vocabulary = ', { } small-title text quadruple row btn-inactive btn-orange btn-green btn-red double <START> header btn-active <END> single <UNK> <PAD>'.split()\n",
        "special_tokens_dict = {'additional_special_tokens': vocabulary}\n",
        "\n",
        "def generate_code(image_path, tokenizer, vit_model, decoder, max_length=512):\n",
        "    # Load and preprocess the image\n",
        "    img_rgb = Image.open(image_path)\n",
        "    img_grey = img_rgb.convert(\"L\")\n",
        "    img_adapted = img_grey.point(lambda x: 255 if x > 128 else 0)\n",
        "    img_stacked = np.stack((img_adapted, img_adapted, img_adapted), axis=-1)\n",
        "    img_stacked_pil = Image.fromarray(np.uint8(img_stacked), mode='RGB')\n",
        "    img_tensor = img_transform(img_stacked_pil)\n",
        "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # Extract image features using ViT model\n",
        "    with torch.no_grad():\n",
        "        image_features = vit_model(img_tensor).last_hidden_state[:, 0, :]\n",
        "\n",
        "    # Prepare the initial input for the decoder\n",
        "    input_ids = tokenizer.encode('<START>', return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate code using the decoder\n",
        "    decoder.eval()\n",
        "    generated_code = []\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output = decoder(input_ids, image_features)\n",
        "        \n",
        "        next_token_id = torch.argmax(output, dim=-1)[:, -1]\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "        \n",
        "        if next_token_id.item() == tokenizer.encode('<END>')[0]:\n",
        "            break\n",
        "\n",
        "        generated_code.append(tokenizer.decode(next_token_id))\n",
        "\n",
        "    return ''.join(generated_code)\n",
        "\n",
        "# Load the tokenizer, ViT model, and decoder\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "special_tokens_dict = {'additional_special_tokens': vocabulary}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').base_model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model.to(device)\n",
        "\n",
        "decoder = GPT2DecoderWithImageFeatures(input_size=768)\n",
        "decoder.gpt.resize_token_embeddings(len(tokenizer))  # Update the GPT2 model with the new tokenizer\n",
        "\n",
        "decoder.load_state_dict(torch.load(\"best_decoder.pth\"))\n",
        "decoder.to(device)\n",
        "\n",
        "# Perform inference on a test image\n",
        "image_path = \"images.png\"\n",
        "generated_code = generate_code(image_path, tokenizer, vit_model, decoder)\n",
        "print(\"Generated code:\\n\", generated_code)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
